{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
        "\n",
        "# Split the dataset into training and validation (90/10 split)\n",
        "train_data = dataset['train']\n"
      ],
      "metadata": {
        "id": "-rH0Lpkbd-xd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer (T5 small in this case)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenization function for the dataset\n",
        "def tokenize_pair(banglish, bengali):\n",
        "    input_text = f\"translate Banglish to Bengali: {banglish}\"\n",
        "    target_text = bengali\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).input_ids\n",
        "    target_ids = tokenizer(target_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).input_ids\n",
        "    return {\"input_ids\": input_ids.squeeze(), \"labels\": target_ids.squeeze()}\n",
        "\n",
        "# Apply tokenization to the dataset\n",
        "train_data = dataset['train'].map(lambda x: tokenize_pair(x['rm'], x['bn']), batched=False)\n",
        "\n",
        "# train_data = train_data.rename_column('input_ids', 'input_ids')\n",
        "# train_data = train_data.rename_column('labels', 'labels')\n"
      ],
      "metadata": {
        "id": "QMw82t5meA-R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model (suitable for translation tasks)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
      ],
      "metadata": {
        "id": "V2kr2T_peEyv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# During training, ensure data is also moved to the same device\n",
        "input_ids = input_ids.to(device)\n",
        "target_ids = target_ids.to(device)\n"
      ],
      "metadata": {
        "id": "u_uSdINPfRum"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Split into train and validation datasets (90% train, 10% validation)\n",
        "train_dataset = train_data.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,                # Reduced epochs\n",
        "    per_device_train_batch_size=16,    # Increased batch size (adjust to GPU memory)\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=5e-4,                # Higher learning rate for faster convergence\n",
        "    warmup_steps=500,                  # Adjust based on dataset size\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,                # Save fewer checkpoints\n",
        "    save_steps=500,                    # Save periodically\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    fp16=True,                         # Enable mixed precision\n",
        "    evaluation_strategy=\"steps\",       # Evaluate during training\n",
        "    eval_steps=500,\n",
        "    report_to=\"none\"                   # Disable W&B logging if not required\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset['train'],\n",
        "    eval_dataset=train_dataset['test'],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ty24u81qeHAY",
        "outputId": "ed30cdff-8b9a-402d-f183-0b3c2ebfcdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/251 00:37 < 1:16:55, 0.05 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}