{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-rH0Lpkbd-xd"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library to load datasets from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Bengali transliteration dataset from Hugging Face\n",
        "# The dataset is identified by the string \"SKNahin/bengali-transliteration-data\"\n",
        "dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
        "\n",
        "# Split the loaded dataset into training data ('train') \n",
        "# This accesses the 'train' split of the dataset for model training\n",
        "train_data = dataset['train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QMw82t5meA-R"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library from Hugging Face Transformers for tokenization\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer for the T5 model (small version)\n",
        "# The tokenizer will process the text inputs into a format suitable for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Define a tokenization function that prepares Banglish and Bengali text for training\n",
        "def tokenize_pair(banglish, bengali):\n",
        "    # Format the input text with a prefix indicating translation task\n",
        "    input_text = f\"translate Banglish to Bengali: {banglish}\"\n",
        "    # The target text is simply the Bengali translation\n",
        "    target_text = bengali\n",
        "    # Tokenize the input text and pad/truncate to max length of 128 tokens\n",
        "    # The output is returned as tensors for model input\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).input_ids\n",
        "    # Tokenize the target text similarly for the labels (output sequence)\n",
        "    target_ids = tokenizer(target_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128).input_ids\n",
        "    # Return the tokenized input and target as a dictionary\n",
        "    return {\"input_ids\": input_ids.squeeze(), \"labels\": target_ids.squeeze()}\n",
        "\n",
        "# Apply the tokenization function to the dataset\n",
        "# We are mapping the 'rm' (Banglish) and 'bn' (Bengali) columns from the dataset to the tokenization function\n",
        "train_data = dataset['train'].map(lambda x: tokenize_pair(x['rm'], x['bn']), batched=False)\n",
        "\n",
        "# Uncomment these lines if you want to rename columns (optional):\n",
        "# train_data = train_data.rename_column('input_ids', 'input_ids')\n",
        "# train_data = train_data.rename_column('labels', 'labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V2kr2T_peEyv"
      },
      "outputs": [],
      "source": [
        "# Import the necessary class from Hugging Face Transformers to load the T5 model\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "# Load a pre-trained T5 model (small version) that is suitable for conditional generation tasks like translation\n",
        "# The \"t5-small\" model has been pre-trained and fine-tuned on various tasks, including translation\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u_uSdINPfRum"
      },
      "outputs": [],
      "source": [
        "# Import the torch library to work with PyTorch tensors and models\n",
        "import torch\n",
        "\n",
        "# Check if a GPU is available for training\n",
        "# If a GPU is available, use it, otherwise fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the chosen device (GPU or CPU)\n",
        "# This ensures that the model runs on the appropriate hardware\n",
        "model.to(device)\n",
        "\n",
        "# During training, move input data and target data to the same device as the model\n",
        "# This ensures that both the model and the data are on the same hardware (either GPU or CPU)\n",
        "input_ids = input_ids.to(device)\n",
        "target_ids = target_ids.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ty24u81qeHAY",
        "outputId": "ed30cdff-8b9a-402d-f183-0b3c2ebfcdbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/251 00:37 < 1:16:55, 0.05 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import necessary classes from Hugging Face Transformers for training\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Split the dataset into training and validation datasets (80% train, 20% validation)\n",
        "# 'train_test_split' creates a test set from the train data for evaluation\n",
        "train_dataset = train_data.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define the training arguments, which configure the training process\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',             # Directory to save model and training results\n",
        "    num_train_epochs=1,                 # Set number of epochs for training (1 epoch in this case for faster iteration)\n",
        "    per_device_train_batch_size=16,     # Set batch size per device (increase based on GPU memory)\n",
        "    per_device_eval_batch_size=16,      # Batch size for evaluation\n",
        "    learning_rate=5e-4,                 # Learning rate (higher for faster convergence)\n",
        "    warmup_steps=500,                   # Number of warmup steps (helps stabilize learning in early stages)\n",
        "    weight_decay=0.01,                  # Apply weight decay to prevent overfitting\n",
        "    save_total_limit=1,                 # Save only the latest checkpoint (limit total saved models)\n",
        "    save_steps=500,                     # Save the model periodically every 500 steps\n",
        "    logging_dir='./logs',               # Directory to save logs\n",
        "    logging_steps=100,                  # Log every 100 steps\n",
        "    fp16=True,                          # Enable mixed precision training for faster processing\n",
        "    evaluation_strategy=\"steps\",        # Evaluate the model at specified steps during training\n",
        "    eval_steps=500,                     # Perform evaluation every 500 steps\n",
        "    report_to=\"none\"                    # Disable reporting to external tools like W&B\n",
        ")\n",
        "\n",
        "# Initialize the Trainer class with model, training arguments, and datasets\n",
        "trainer = Trainer(\n",
        "    model=model,                        # The model to train\n",
        "    args=training_args,                 # The training arguments configured above\n",
        "    train_dataset=train_dataset['train'],  # Use the training split for training\n",
        "    eval_dataset=train_dataset['test'],   # Use the test split for evaluation\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
